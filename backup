# import streamlit as st
# from streamlit_webrtc import webrtc_streamer
# import av
# import cv2
# import numpy as np
# import mediapipe as mp
# from keras.models import load_model
# import random
# import time
# import os
# from twilio.rest import Client
# import config
# import base64
#
# # Find your Account SID and Auth Token at twilio.com/console
# # and set the environment variables. See http://twil.io/secure
# account_sid = config.TWILIO_ACCOUNT_SID
# auth_token = config.TWILIO_AUTH_TOKEN
# client = Client(account_sid, auth_token)
#
# token = client.tokens.create()
# path = os.path.dirname(__file__)
#
# model  = load_model(os.path.join(path,"model_4.h5"))
# label = np.load(os.path.join(path,"labels.npy"))
# holistic = mp.solutions.holistic
# hands = mp.solutions.hands
# holis = holistic.Holistic()
# drawing = mp.solutions.drawing_utils
# st.header("Emotion Based Music Recommender")
# '''op = Options()
# ext = os.path.join(path,'extension_5_2_6_0.crx')
# op.add_extension(ext)'''
#
#
# if "run" not in st.session_state:
# 	st.session_state["run"] = "true"
#
# try:
# 	emotion = np.load(os.path.join(path,"emotion.npy"))[0]
# except:
# 	emotion=""
#
# if not(emotion):
# 	st.session_state["run"] = "true"
# else:
# 	st.session_state["run"] = "false"
#
# class EmotionProcessor:
# 	def recv(self, frame):
# 		frm = frame.to_ndarray(format="bgr24")
#
# 		##############################
# 		frm = cv2.flip(frm, 1)
#
# 		res = holis.process(cv2.cvtColor(frm, cv2.COLOR_BGR2RGB))
#
# 		lst = []
#
# 		if res.face_landmarks:
# 			for i in res.face_landmarks.landmark:
# 				lst.append(i.x - res.face_landmarks.landmark[1].x)
# 				lst.append(i.y - res.face_landmarks.landmark[1].y)
#
# 			if res.left_hand_landmarks:
# 				for i in res.left_hand_landmarks.landmark:
# 					lst.append(i.x - res.left_hand_landmarks.landmark[8].x)
# 					lst.append(i.y - res.left_hand_landmarks.landmark[8].y)
# 			else:
# 				for i in range(42):
# 					lst.append(0.0)
#
# 			if res.right_hand_landmarks:
# 				for i in res.right_hand_landmarks.landmark:
# 					lst.append(i.x - res.right_hand_landmarks.landmark[8].x)
# 					lst.append(i.y - res.right_hand_landmarks.landmark[8].y)
# 			else:
# 				for i in range(42):
# 					lst.append(0.0)
#
# 			lst = np.array(lst).reshape(1,-1)
#
# 			pred = label[np.argmax(model.predict(lst))]
#
# 			cv2.putText(frm, pred, (50,50),cv2.FONT_ITALIC, 1, (255,0,0),2)
#
# 			np.save("emotion.npy", np.array([pred]))
#
# 		##############################
#
# 		return av.VideoFrame.from_ndarray(frm, format="bgr24")
#
# webrtc_streamer(key="key", desired_playing_state=True,
# 				video_processor_factory=EmotionProcessor,
# 				media_stream_constraints={"video": True, "audio": False},rtc_configuration={
#       "iceServers": token.ice_servers
#   })
# '''def most_common(List):
#     return(mode(List))
#
#
# def song_url(song_id: str) -> str:
#     return ytm.utils.url_ytm('watch', params = {'v': song_id})
#
# def search(query: str) -> str:
#     k = random.randint(0,10)
#     return song_url(ytm.YouTubeMusic().search_songs(query)['items'][k]['id'])
#
#
# class play:
# 	def __init__():
# 		global driver
# 	def close():
# 		driver.close()
# 	def play_music():
# 		st_player(p,playing=True)
# 		driver = webdriver.Chrome(options=op)
# 		driver.get(p)
# 		driver.implicitly_wait(3)
# 		driver.find_element(By.XPATH, '//*[@id="play-pause-button"]').click()
# 		time.sleep(30)
# 		for i in range(0,1):
# 			driver.find_element(By.XPATH, '//*[@id="left-controls"]/div/tp-yt-paper-icon-button[5]').click()
# 			time.sleep(150)
# 			if btn2:
# 				pl.close()
# 		driver.close()
#
#
# emo = emotion
# btn1 = st.button("Start")
# btn2 = st.button("Stop")
# pl=play
# x = EmotionProcessor
# if btn1 and emo != 'happy':
# 	while emo != 'happy' or not btn2:
# 		emo = ''
# 		l=[]
# 		p = search('top random')
# 		print(p)
# 		pl.play_music()
# 		if btn2:
# 			pl.close()
# 			break
# 		for i in range(1,20):
# 			EmotionProcessor
# 			emotion = np.load("emotion.npy")[0]
# 			l.append(emotion)
# 			emo = most_common(l)
# 		if emo == 'happy':
# 			pl.close()'''
#
# l1 = ['Manike_192(PagalWorld.com.se).mp3','Maan Meri Jaan_192(PagalWorld.com.se) (1).mp3']
#
# st.write("# Auto-playing Audio!")
# def autoplay_audio(file_path: str):
#     with open(file_path, "rb") as f:
#         data = f.read()
#         b64 = base64.b64encode(data).decode()
#         md = f"""
#             <audio controls autoplay="true">
#             <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
#             </audio>
#             """
#         st.markdown(
#             md,
#             unsafe_allow_html=True,
#         )
# placeholder = st.empty()
# x=0
# while x<4:
# 	emo = np.load("emotion.npy")[0]
# 	st.write(emo)
# 	if emo =='happy':
# 		placeholder.auto(l1[0])
# 	else:
# 		placeholder.auto(l1[1])
# 	time.sleep(20)
# 	x=x+1
#
